---
title: "Coding and Recoding Scaled Items"
linktitle: "Week 11: Coding and Recoding Scaled Items"
output:
  blogdown::html_page:
    toc: true
menu:
  example:
    parent: Examples
    weight: 3
type: docs
weight: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(widgetframe_self_contained = TRUE) 
knitr::opts_knit$set(root.dir = getwd())

library(tidyverse)
library(fivethirtyeight)
library(psych)
library(knitr)
library(kableExtra)
```

<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Take This

First things first, please go and take the [Big Five Personality Test](https://openpsychometrics.org/tests/IPIP-BFFM/){target="_blank"}. What do you think?

## Read Up

Now go and read this short synopsis on the test from [FiveThirtyEight](https://projects.fivethirtyeight.com/personality-quiz/){target="_blank"}.

## Data Files

Please download the files below. Note you may have to right click and download each file if your browser renders `.csv` files automatically:

- [`Big5data.csv`](/data/Big5data.csv)
- [`Big5codebook.csv`](/data/Big5codebook.csv)
- [`Big5measures.csv`](/data/Big5measures.csv)

## Prerequisites {-}

Open up R and create a new script by going to **File > New File > R Script**. Save this in an easily accessible folder. Now take the files `Big5data.csv`, `Big5codebook.csv` and `Big5measures.csv` and drop them all in the same folder as your new script. Until you get use to directories and file paths, consider keeping both your R script and data sets in the same folder. It just makes life easier.

Before we load the libraries, we're going to grab a package called `surveytools2` that is not on CRAN (the main R repository of packages). In fact, many packages aren't on CRAN since the process to get accepted is relatively hectic! In any case, there is a command within the `surveytools2` package that will make our lives easier which we'll go over shortly. To install, please run the following commands

```{r eval = FALSE}
install.packages("remotes")

remotes::install_github("peterhurford/surveytools2")
```

Now go ahead and load up the following libraries or download and load up if needed

```{r}
library("tidyverse")
library("psych")
library("corrr")
library("surveytools2")
```

and then set the working directory to the location of the script by running

```{r eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

or by selecting **Session > Set Working Directory > To Source File Location**.

## Data Files

We'll be looking at a very truncated data set from the **2018 BigFive Personality Test**. The full data set is a huge file with 1 million answers to the 50 personality items! You can access it [here](
https://openpsychometrics.org/_rawdata/IPIP-FFM-data-8Nov2018.zip) but be warned that opening a file that size can take a long time or may even crash some computers depending on their processing power and RAM. It is a tab separated `.csv` file. 

### Side Note

I am not certain how to load it in Excel but in R you can use

```{r eval=FALSE}
toobig_five_data <- read_delim("data-final.csv", delim="\t") 
```

to save it to the `big_five_data` variable which you can access by running

```{r eval=FALSE}
toobig_five_data
```

At this point you probably want to just open up the `.csv` files in Excel and if that is what works, then by all means. While I try to run everything in R, sometimes its just easier to double click it. The problem really lies in the size of a data set. The larger the data set, the more power and time it takes for excel to load it up both into its program and to show it to you. Its really that second part that's the killer! With R you avoid the latter step which means your data set loads up within seconds, rather than minutes. If you try loading the original data set in both R and Excel, the difference is very apparent. 

In any case, we will be working with data strictly in R but never feel bad about opening a data set using another application like Excel. For all of my snarkiness towards anything Office, Excel has some nice qualities.

## Loading a Local Data Set

Let's load the data set, codebook, and measures

```{r eval=FALSE}
big_five_data <- read_csv("Big5data.csv")
```

```{r echo=FALSE}
big_five_data <- read_csv("data/Big5data.csv")
```

```{r eval=FALSE}
big_five_codebook <- read_csv("Big5codebook.csv")
```

```{r echo=FALSE}
big_five_codebook <- read_csv("data/Big5codebook.csv")
```

```{r eval=FALSE}
big_five_measures <- read_csv("Big5measures.csv")
```

```{r echo=FALSE}
big_five_measures <- read_csv("data/Big5measures.csv")
```

and take a look at each

```{r}
big_five_data %>%
  head()
```

with respondents self-report under the five factors Extroversion (`EXT`), Agreeableness (`AGR`), Conscientiousness (`CSN`), Emotional Stability (`EST`), and Intellect/Imagination (`OPN`).

```{r}
big_five_codebook
```

```{r}
big_five_measures
```

For this walkthrough, let's just concentrate on the Extroversion items which are given by `EXT.` That means we should probably pair down both the data set and codebook. For data sets with multiple columns, we can use the `select()` syntax. Moreover since we are looking at all of the columns that have `EXT`, we can pull only those using the `starts_with()` command. For more examples of options that can be used with `select()`, take a look [here](https://dplyr.tidyverse.org/reference/select.html){target="_blank"}. 

```{r}
big_five_data_EXT <- big_five_data %>%
                      select(starts_with("EXT"))
```

and then we'll do something similar to the codebook. While we can't simply select columns because all of the variables are in a single column. Much like in Excel, we can filter rows. To do this in R, we use the `filter()` command.

```{r}
big_five_codebook_EXT <- big_five_codebook %>%
                          filter(str_detect(ID, "EXT"))
```

```{r}
big_five_data_EXT

big_five_codebook_EXT
```

## Reverse Coding

Sometimes items can be worded ‘backwards’. In these situation, scales need to be reversed to point in the same direction. 

### Why Does This Matter?

From a methodological perspective, if all of your scales are not pointed the right direction you will likely get confusing or conflicting figures, or even worse report false outcomes resulting in Type I and Type II errors! Of course you may also just get lucky and your findings will be spot on. However, if you don't really want to leave it up to chance to give you the actual outcomes of your survey, then **reverse coding** is something you should consider before performing any analysis. And yes this absolutely still applies if you are certain that everything was perfect before your survey was administered.

### Example

For example both these questions might be used to measure the same underlying construct or idea.

1. I really like being alone
2. I really *don't* enjoy large gatherings

In this situation, if we used the same response scale for each participants’ responses they would simply cancel out. 

When aligning the direction, we typically reverse code the one that has a negative, aka the negatively-keyed item. In this case is the second example since it includes the word *don't*. There are exceptions to this rule, but they are narrowly defined. For example, someone who states that they "really *don't* enjoy large gatherings" doesn't automatically imply they like being alone. In fact, there are arguably an infinite number of possibilities of things they do like! The only time you can be code all items in either direction are in circumstances where there is a binary choice (e.g. Yes/No questions).

But wait...there's more! You also have to consider the intent of the questions and the participants. This second and final check is crucial to reporting reliable and consistent results.<br />
:::

Getting back to the main data set, let's again look at the codebook.  

```{r}
big_five_codebook_EXT
```

Well it certainly looks like `EXT2`, `EXT4`, `EXT6`, `EXT8` and `EXT10` are all negative in the context of of assessing extroversion. Furthermore while `EXT9` includes the term *don't*, it doesn't orient the statement in a negative way so it stays the way it is. 

Remember that command we needed from the `surveytools2` package? Well it was to reverse code in a tidy way

```{r}
big_five_data_EXT_rev <- big_five_data_EXT %>%
                        mutate(EXT2 = reverse_code(EXT2),
                               EXT4 = reverse_code(EXT4),
                               EXT6 = reverse_code(EXT6),
                               EXT8 = reverse_code(EXT8),
                               EXT10 = reverse_code(EXT10))

big_five_data_EXT_rev
```

Let's take a look at a sample of the first six rows between the two tables to see if the columns were indeed reverse coded.

<div align="center">

```{r, echo = FALSE}
big_five_data_EXT %>% head()
```

```{r, echo = FALSE}
big_five_data_EXT_rev %>% head()
```

</div>

That looks correct! Notice that in the <a style="color:	#FFFFFF; background-color: #419873">green</a> columns, the numbers flip along the neutral measure (3), or maybe a better observation is that the numerical complement is correct if adding two corresponding cells equals 5. If you have had any upper level mathematics courses, you may notice these as a modulo or mod 5 system.

## Computing the Internal Consistency 

There are many approaches when assessing reliability, namely those associated with interviews, experiments, surveys, trials, etc. Internal consistency is foremost one of many existing methods used to measure reliability, and it is typically useful in assessing items (e.g from a protocol, questionnaire, test, etc). It essentially **estimates to what degree total scores on an instrument would change or vary if different items were used**. Here are four but really three approaches used by a majority of practitioners:

### Composite Reliability

Getting a bit ahead of ourselves, composite reliability is based on the factor lodgings in a confirmatory factor analysis (CFA). While we do cover exploratory factor analysis (EFA) in this course, we simply don't have bandwidth time to address CFAs as well. Please consider learning about them as they are used to test how well the measured variables represent the number of constructs within an or across instrument(s).

### Cronbach’s alpha

By far the most utilized approach, it is also very procedural which is typically viewed as beneficial by many evaluators. There are many ways to tell R to compute alpha $\left(\alpha\right)$ but by far the simplest may be to use the `psych` package

```{r}
psych::alpha(big_five_data_EXT_rev)
```

There is a lot there and most of those outcomes are useful, but for reliability we really care about the `raw_alpha`, or the *the standardized alpha based upon the correlations*. For a raw $\alpha$ score, the following breakdown is widely accepted but not without controversy. 

<div align="center">

| Cronbach's Alpha       | Measure of Internal Consistency |
|------------------------|---------------------------------|
| $\alpha \geq 0.90$     | Excellent                       |
| $0.70 >\alpha\geq0.90$ | Good                            |
| $0.80 >\alpha\geq0.70$ | Acceptable                      |
| $0.70 >\alpha\geq0.60$ | Poor                            |
| $\alpha < 0.50$        | Unacceptable                    |
|                        |                                 |

</div>

Note that the thresholds themselves do not have any logical grounding, rather what is an acceptable threshold may be situation dependent. 

If you’d like to access the alpha value easily, implement this

```{r}
psych::alpha(big_five_data_EXT_rev)$total$raw_alpha
```

which for a **quick and dirty** measure of reliability seems to be pretty Good!


### Inter-item Correlation

This is foremost an **average** which uses correlations. 

```{r}
big_five_data_EXT_rev %>%
  correlate()
```

The package gives an `NA` for variables that try to correlate with themselves. By default each variable should be perfectly correlated with itself but who cares?  With the diagonal owned by `NA`s, we can obtain the average correlation of each item with all others by computing the means for each column `EXT1` - `EXT10`.

```{r}
inter_measure  <- big_five_data_EXT_rev %>% 
                    correlate() %>% 
                    select(-term) %>% 
                    colMeans(na.rm = TRUE)

inter_measure 
```

We can check the variables that are more strongly correlated with the other items in the `EXT` group. The same information is presented by arranging categories and descending internal consistency measures, respectively.

```{r}
big_five_correlations_cat <- data.frame(inter_measure) %>%
                               rownames_to_column() %>%
                               as_tibble() %>%
                               arrange(desc(inter_measure)) 

big_five_correlations_num <- big_five_correlations_cat %>%
                               left_join(big_five_codebook, c("rowname" = "ID"))

big_five_correlations_cat

big_five_correlations_num 
```

Note we'll talk about a `left_join()` and other types of `joins` next week but for now if you can't wrap your head around what happened in that step, that's fine! If you want to get a head start on `joins`, go over to [STAT545](https://stat545.com/join-cheatsheet.html) and as usual, peers, the Internet and I are available to help. 

In any case, it looks like `EXT4`, `EXT5`, `EXT10`, and `EXT7` have the strongest correlation. The list below has been arranged to represent the data in descending order of measure.

<div align="center">
```{r, echo = FALSE}
big_five_correlations_num %>%
  rename("Variable" = "rowname") %>%
  rename("Internal Consistency Measure" = "inter_measure") %>%
kable(align = "c") %>%
  kable_styling(full_width = FALSE, 
                font_size = 13.0,
                position = "center",
                bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%          
                row_spec(1, color='white', background='#5AA85C')  %>%        
                row_spec(2, color='white', background='#599D68')  %>% 
                row_spec(3, color='white', background='#56916F')  %>% 
                row_spec(4, color='white', background='#538572')  %>%
                row_spec(5, color='white', background='#4F7972')  %>%
                row_spec(6, color='white', background='#4B6C6D')  %>%
                row_spec(7, color='white', background='#465B61')  %>%
                row_spec(8, color='white', background='#404C55')  %>%
                row_spec(9, color='white', background='#393F49')  %>%
                row_spec(10, color='white', background='#32333E')
```
</div>

and the overall average inter-item correlation can be found by

```{r}
mean(inter_measure)
```


### Item-total Correlation

The item-total correlation is similar to the correlation described previously. We are calculating the average but this time, we're doing it by row rather than by column. To do that we sum 

```{r}
agg_scores <- big_five_data_EXT_rev %>% 
  mutate(score = rowMeans(select(.,1:10))) %>% 
  correlate() %>% 
  focus(score) %>%
  arrange(desc(score))

agg_scores
```

Well that is quite enough for now. There are other ways to assess reliability. As you move to the deliverable for this week, keep in mind that none of these could yield good measures for internal consistency. You may have to look around the internet for other R commands and packages that may be a better fit. As usual, you always have your peers and me in times of frustration with R.
