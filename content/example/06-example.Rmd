---
title: "From a Certain Poimt of View... A Certain Point of View?"
linktitle: "Week 6: From a Certain Poimt of View... A Certain Point of View?"
output:
  blogdown::html_page:
    toc: true
menu:
  example:
    parent: Examples
    weight: 4
type: docs
weight: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(widgetframe_self_contained = TRUE) 
knitr::opts_knit$set(root.dir = getwd())

library(tidyverse)
library(psych)
library(ggcorrplot)
library(DT)
library(polycor)
library(knitr)
library(kableExtra)
```

<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# Principal Component Analysis (PCA)

## Take This

First things first, please go and take the [Nerdy Personality Attributes Scale](https://openpsychometrics.org/tests/NPAS/){target="_blank"}. What do you think?

## Read Up

- First go and read this prepring on [PsyArXiv](https://psyarxiv.com/xt8ey/){target="_blank"} which discusses uses an EFA on a sample of the Nerdy Personality Attributes Scale.
- Second download the entire study at [OSF](https://osf.io/5njdx/) and going to `Paper > Download as zip`.

## Data Files

Please download all the files below. Note you may have to right click and download each file if your browser renders csv files automatically:

- [`nerdyData.csv`](/data/nerdyData.csv)
- [`nerdyCodebook_PM.csv`](/data/nerdyCodebook_PM.csv)
- [`nerdyMeasures_PM.csv`](/data/nerdyMeasures_PM.csv)
- [`nerdyCodebook_TIPI.csv`](/data/nerdyCodebook_TIPI.csv)
- [`nerdyMeasures_TIPI.csv`](/data/nerdyMeasures_TIPI.csv)


## Prerequisites {-}

Using the exact same process the previous weeks, please open up R and create a new script by going to **File > New File > R Script**. Save this in an easily accessible folder. Now take the files `nerdyData.csv`, `nerdyCodebook_PM.csv`, `nerdyCodebook_TIPI.csv`, `nerdyMeasures_PM.csv`, and `nerdyMeasures_TIPI.csv` and drop them all in the same folder as your new script. Getting comfortable with  directories and file paths is not necessaroly easy for noices so please consider keeping both your R script and data sets in the same folder.

Before we load the libraries, we're going to grab a package called `psych` that is on CRAN. Remember that you only need to do this is if it was not done last week! 

To install, please run the following command

```{r eval = FALSE}
install.packages("psych", dependencies = TRUE)
```

or you can simply use the dropdown menu by going to **Tools > Install Packages** and simply type in `patchwork`. Remember to have **Install dependencies** checkmarked! 

```{r eval=FALSE}
library("tidyverse")
library("patchwork")
```

Now set the working directory to the location of the script by running

```{r eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

or by selecting **Session > Set Working Directory > To Source File Location**.

## PCA

Suppose you are conducting a survey and you want to know whether the items in the survey have similar patterns of responses, do these items “hang together” to create a construct? This is the underlying principle of a **factor analysis**, or **FA**. 

The basic assumption of an fa is that for a collection of observed variables there are a set of underlying variables called factors (smaller than the observed variables), that can explain the interrelationships among those variables.

In this session, we're going to conduct what is known as a single-factor **explanatory factor analysis**, or **EFA**. When you’re using this in the real world, be sure to use a dataset that only contains item responses in a numerical format (e.g. those form a Likert scale question) - other types of data will cause errors and/or incorrect results. In the Generic Conspiracist Beliefs Scale, or GCBS dataset, these are participant responses to 15 items from the Generic Conspiracist Beliefs Scale, which is aptly designed to measure conspiracist beliefs.

## Loading a Local Data Set

Like before, Let's load the data set, codebook, and measures 

```{r eval=FALSE}
conspiracy_data <- read_csv("conspiracyData.csv")
```

```{r echo=FALSE}
conspiracy_data <- read_csv("data/conspiracyData.csv")
```

```{r eval=FALSE}
conspiracy_codebook <- read_csv("conspiracyCodebook.csv")
```

```{r echo=FALSE}
conspiracy_codebook <- read_csv("data/conspiracyCodebook.csv")
```

```{r eval=FALSE}
conspiracy_measures <- read_csv("conspiracyMeasures.csv")
```

```{r echo=FALSE}
conspiracy_measures <- read_csv("data/conspiracyMeasures.csv")
```


and then look at the data, codes, and measures themselves using

```{r]}
conspiracy_data %>%
  head()
```

```{r}
conspiracy_codebook %>%
  head()
```

```{r}
conspiracy_measures
```

or you can use the `datatable()` command from the `DT` package for a more Excel like feel

```{r}
datatable(conspiracy_data)
```

So how big is this data set? Well if you love counting, go start or you could simply use the following

```{r}
dim(conspiracy_data)
```

Now that's 2495 rows by 15 columns which is a pretty big data set.

## Conducting an EFA

Conducting an EFA is relatively straight forward in R and can be done by using the `fa()` function from the `psych` package. 

```{r}
EFA <- fa(conspiracy_data)
```

and then to view the results 

```{r}
EFA
```

### Note
Well that's a lot of information. While every item on here is important depending on need, we're going to skip the explanation of everything besides the column `MR1` in this course because without some solid background in statistics, the explanation of other item here are difficult to describe. In fact, a majority are not even relevant for our purposes.

## Viewing and Visualizing Factors

Each result using the `fa()` command actually gives you a list, and each element of the list contains specific information about the analysis, including factor loadings. *Factor loadings* represent the strength and directionality of the relationship between each item and the underlying factor, and they can range from -1 to 1. We can see this by looking only at the first column

```{r}
EFA$loadings
```

You can also create a diagram of loadings. The `fa.diagram()` command takes the outpit from `fa()` and creates a diagram, called a *path diagram* to show the items’ loadings ordered from strongest to weakest. This type of visualization can be a helpful way to represent your results.

```{r}
fa.diagram(EFA)
```

### Note
So why can't we just use pipes and `ggplot` here? Well the main problem can be found using this command

```{r}
class(EFA)
```

Notice that the `psych` package has its own data type called  "psych". Tidy involves data that is neatly in rows and columns. In any case, we'll stick to the default format for now.

## Interpreting Factor Scores

The `EFA` object we've created also contains a named list element, scores, which contains factor scores for each person. These factor scores are an indication of how much or how little of the factor each person is thought to possess. Please note that factor scores are not computed for participants with missing data.

The EFA object also contains a named list element, scores, which contains factor scores for each person. These factor scores are an indication of how much or how little of the factor each person is thought to possess. Factor scores are not computed for participants with missing data.

To see an example, lets first look at the first six rows of the data set

```{r}
head(conspiracy_data)
```

or we can have a more Excel like version using 

```{r}
datatable(head(conspiracy_data))
```


