---
title: "Conspiracies"
linktitle: "Week 4: Conspiracies"
output:
  blogdown::html_page:
    toc: true
menu:
  walkthroughs:
    parent: Walkthroughs
    weight: 4
type: docs
weight: 1
editor_options: 
  chunk_output_type: console
---
<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<style>
  .hvr-sweep-to-left {
    display: inline-block;
    vertical-align: middle;
    -webkit-transform: perspective(1px) translateZ(0);
    transform: perspective(1px) translateZ(0);
    box-shadow: 0 0 1px rgba(0, 0, 0, 0);
    position: relative;
    -webkit-transition-property: color;
    transition-property: color;
    -webkit-transition-duration: 0.25s;
    transition-duration: 0.25s;
  }

.hvr-sweep-to-left:before {
  content: "";
  position: absolute;
  z-index: -1;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: #761756;
    -webkit-transform: scaleX(0);
  transform: scaleX(0);
  -webkit-transform-origin: 100% 50%;
  transform-origin: 100% 50%;
  -webkit-transition-property: transform;
  transition-property: transform;
  -webkit-transition-duration: 0.3s;
  transition-duration: 0.3s;
  -webkit-transition-timing-function: ease-out;
  transition-timing-function: ease-out;
}

.hvr-sweep-to-left:hover, .hvr-sweep-to-left:focus, .hvr-sweep-to-left:active {
  color: white;
}

.hvr-sweep-to-left:hover:before, .hvr-sweep-to-left:focus:before, .hvr-sweep-to-left:active:before {
  -webkit-transform: scaleX(1);
  transform: scaleX(1);
}
</style>

```{r setup, include=FALSE,purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(widgetframe_self_contained = TRUE) 
knitr::opts_knit$set(root.dir = getwd())

library(tidyverse)
library(fivethirtyeight)
library(psych)
library(DT)
library(reactable)
library(knitr)
library(kableExtra)
library(htmlwidgets)
library(htmltools)
```

## Take This

First things first, please go and take the [Generic Conspiracist Beliefs Scale](https://openpsychometrics.org/tests/GCBS/){target="_blank"}. 

## Read Up

Now go and read this short synopsis on this and other tests to assess what drives conspirators at [Psychology Today](https://www.psychologytoday.com/us/blog/experimentations/201810/what-makes-conspiracy-theorists-tick){target="_blank"}^[I am personally fond of the BSR.]. 

## Data Files

Please download the files we'll need.

```{r echo=FALSE,eval=TRUE,message=FALSE,purl=FALSE}
loc <- here::here("static", "data", "EDP611Week4Data&Script&Paper.zip")

downloadthis::download_file(
  path = loc,
  output_name = "Week 4 Data and Script and Paper",
  button_label = "Download Data and Script and Paper",
  button_type = "default",
  has_icon = TRUE,
  icon = "fa fa-save",
  class = "hvr-sweep-to-left"
  )
```



## Prerequisites {-}

Like last week, open up Rstudio and create a new script by going to **File > New File > R Script**. Save this in an easily accessible folder. Now unzip this week's data set and take the files - `ConspiracyData.csv`, `ConspiracyCodebook.csv`, `ConspiracyMeasures.csv`, and `04-walkthrough` - and drop them all in a folder of their own. The Watkins (2018) article is a well written and handy guide to an otherwise dense statistical approach we'll be using today.

Before we load the libraries, we're going to grab a package called `psych` that is on CRAN. To install it, please run the following command in your console

```{r eval = FALSE}
install.packages("psych", dependencies = TRUE)
```

or you can simply use the dropdown menu by going to **Tools > Install Packages** and simply type in `psych`. Remember to have **Install dependencies** checkmarked! 

If you would like to know more about the `psych` package and gain access to an extensive set of resources for personality and psychological tests, take a look at the [Personality Project](https://personality-project.org/r/psych/){target="_blank"}.

Now please go ahead and load up the following libraries or download and load if needed

```{r eval=FALSE}
library("tidyverse")
library("psych")
library("reactable")
library("polycor")
```

Then set the working directory to the location of the script by running

```{r eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

in your console or by selecting **Session > Set Working Directory > To Source File Location**.

## Data Files

We'll be looking at responses from a sample of 2449 people who took this test in 2016. 

## Loading a Local Data Set

To load the data set, codebook, and measures, run the following

```{r eval=FALSE}
conspiracy_data <- read_csv("ConspiracyData.csv")
```

```{r echo=FALSE, purl=FALSE}
conspiracy_data <- read_csv(here::here("static", "data", "ConspiracyData.csv"))
```

```{r eval=FALSE}
conspiracy_codebook <- read_csv("ConspiracyCodebook.csv")
```

```{r echo=FALSE, purl=FALSE}
conspiracy_codebook <-
  read_csv(here::here("static", "data", "ConspiracyCodebook.csv"))
```

```{r eval=FALSE}
conspiracy_measures<- read_csv("ConspiracyMeasures.csv")
```

```{r echo=FALSE, purl=FALSE}
conspiracy_measures<-
  read_csv(here::here("static", "data", "ConspiracyMeasures.csv"))
```

and take a look at each

```{r}
conspiracy_codebook %>%
  head()
```

```{r}
conspiracy_measures
```

or to see a nicer table that is also interactive, you can use the `reactable()` command from the aptly named `reactable` package for a more Excel like feel and functionality.

```{r}
reactable(conspiracy_measures)
```

```{r}
reactable(conspiracy_codebook)
```

Well that is really long big and an annoyance if you want to find something. Let's fix it by specifying the number of rows that appear and give it a search box.

```{r}
reactable(conspiracy_codebook,
          searchable = TRUE, 
          defaultPageSize = 3)
```

Now that is certainly better but if you don't like it, just change the `3` in `defaultPageSize = 3` to whatever you want.

Moving on, what if the data set is big? Then we might have an even bigger problem in how to display it. Let's remind ourselves of the dimensions

```{r}
dim(conspiracy_data)
```

That's 2495 rows by 15 columns which is a pretty big data set and unlike the codebook, sometimes its nice to have options when it comes to looking at the number of rows and highlight the one I'm on. It may also be nice to save it as as something easier to recall like a word (aka a variable). You can probably see where this is going...

```{r}
thing <- reactable(conspiracy_data,
                   searchable = TRUE, 
                   defaultPageSize = 5,
                   showPageSizeOptions = TRUE,
                   highlight = TRUE)

thing
```

That's a bit better, though its certainly not perfect. There are a lot of things you can change about the table to make it easier to use. If you want to know more take a look at this [vignette](https://glin.github.io/reactable/index.html){target="_blank"}. Don't be scared off by the syntax. Remember its open source so just copy and paste. Eventually if you want to change certain aspects, just start tweaking what's already there.

For example, I think that reactable's ability toggle a table dark and light mode would be something nice to have. When you initially look at the [syntax](https://glin.github.io/reactable/articles/examples.html#nested-selectors){target="_blank"}, it looks pretty complicated, that is until you realize all you really have to do is copy the theme and functionality

```{r}
theme <- reactableTheme(
  style = list(".dark &" = list(color = "#fff", background = "#282a36")),
  cellStyle = list(".dark &" = list(borderColor = "rgba(255, 255, 255, 0.15)")),
  headerStyle = list(".dark &" = list(borderColor = "rgba(255, 255, 255, 0.15)")),
  paginationStyle = list(".dark &" = list(borderColor = "rgba(255, 255, 255, 0.15)")),
  rowHighlightStyle = list(".dark &" = list(background = "rgba(255, 255, 255, 0.04)")),
  pageButtonHoverStyle = list(".dark &" = list(background = "rgba(255, 255, 255, 0.08)")),
  pageButtonActiveStyle = list(".dark &" = list(background = "rgba(255, 255, 255, 0.1)"))
) # This was copied

togglething <- reactable(conspiracy_data,
                          searchable = TRUE, 
                          defaultPageSize = 5,
                          showPageSizeOptions = TRUE,
                          highlight = TRUE,
                          theme = theme) # I only added this last line

tags$button(onclick = "document.querySelector('.themeable-tbl').classList.toggle('dark')",
            "Toggle light/dark") # This was copied.

div(class = "themeable-tbl dark", togglething) # This was copied and only the variable name changed.
```

<br>
With very little effort, I have this table with a nice option to fool around with the aesthetics a bit and it all took about 10 seconds. This isn't unique! You have this ability with nearly everything in R. So don't code...just copy and tweak. You learn R by tinkering and using it, not by having nightmares about coding^[Yuck]. OK enough of that, let's get to our method of the day.

## Method: Explanatory Factor Analysis

Suppose you are conducting a survey and you want to know whether the items in the survey have similar patterns of responses, do these items *hang out together* to create a construct? This is the underlying principle of a **factor analysis**, or **FA**. 

The basic assumption of a **FA** is that for a collection of observed variables, there are a set of underlying variables called factors which are smaller than those observed variables that can explain the interrelationships among those variables.

Moving forward, we're going to conduct what is known as a single-factor **exploratory factor analysis**, or **EFA**. When youâ€™re using this in the real world, be sure to use a dataset like this one, in that it only contains item responses in a numerical format - other types of data will cause errors and/or incorrect results. In the GCBS the scale is numeric so our dataset will do.

In a nutshell, an EFA is just a statistical method used to uncover underlying categorizations, groups, structures often hidden within a data set. This is done by assessing measures and could easily serve as your group level data! As a result, you can measure the internal reliability of a measure as well.

### A Few Things
When you have an instrument with multiple scales, there are three standard approaches, of which we will only look at the last.

```{r, echo=FALSE}

lst <- tibble(
  
  approach = c("Performed accounting for multiple scales",
               "Performed on scales that have been reduced",
               "Performed on grouped items with like scales"
               ),
  
  complexity = c("Most",
                 "Less",
                 "Minimal"
                 ),
  
  information = c("Information is lossless",
                  "Information gathered from higher order scales is lost due to reduction",
                  "Only information between items with the same scaling is retained"
                  ),
  
  get = c("Between items of the same AND different scales",
         "Between items of the same AND different scales at the lowest scaling",
          "Between items of the same scale"
         )
  
)

```

<center>
```{r, echo=FALSE,purl=FALSE}
lst %>%
  kbl(col.names = c("EFA approach", "Complexity", "Retention", "Description"), 
      "html", 
      escape = FALSE,
      align = 'llll') %>%
  kable_styling(full_width = FALSE) %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "10em") %>%
  column_spec(3, width = "30em") %>%
  column_spec(4, width = "30em") %>%
  row_spec(0, extra_css = 'vertical-align: middle !important;') %>%
  row_spec(1, extra_css = 'vertical-align: middle !important;') %>%
  row_spec(2, extra_css = 'vertical-align: middle !important;') %>%
  row_spec(3, extra_css = 'vertical-align: middle !important;') 
```
</center>

Given that we'll be only concentrating on the latter, you may have to run more than one EFA. After conducting the EFA, there are some things to consider if and when implementing your next one whihc we will cover at the end.

### Example

Theoretically a five-point likert scale will have five underlying factors, or one for each choice. However in practice, this is rarely the case and we tend to treat the scale as an upper limit - ergo we can't have more than five factors. So to decide on the number of factors, we use what is called a scree plot. Let's do this for the first ten rows of our data set.

```{r}
conspiracy_data_10 <- conspiracy_data %>%
                      head(n=10)
```

```{r, warning=FALSE}
scree(conspiracy_data_10)
```

We're going to ignore the *y*-axis and just look at where the open circle seems to cross that horizontal line. It looks like this happens around 3, so we can that there are likely three factors.

Conducting an EFA is relatively straight forward in R, and can be performed in any number of packages. In our case, we'll use the popular `psych` package and the command `fa()` where we define the number of factors by `nfactors`. 


```{r, warning=FALSE}
EFA <- fa(conspiracy_data_10,
          nfactors = 3)
```

and then to view the results 

```{r}
EFA
```

### Viewing and Visualizing Factors

Each result using the `fa()` command actually gives you a list, and each element of the list contains specific information about the analysis, including factor loadings. *Factor loadings* represent the strength and of the relationship between each item and the underlying factor, and they can range from -1 to 1. We can see this by looking only at the first column

```{r}
EFA$loadings
```

You can also create a diagram of loadings. The `fa.diagram()` command takes the output from `fa()` and creates a diagram, called a *path diagram* to show the itemsâ€™ loadings ordered from strongest to weakest. This type of visualization can be a helpful way to represent your results where the factor loading are represented on the arrows going from each factor to their respective items.

```{r}
fa.diagram(EFA)
```

For those of you who have some statistical background, *factor loadings* show the correlation between the observed score and the latent score. Generally, the higher the better since the square of factor loading can be directly translated as item reliability. The other number between the factors is called a *factor correlation* that I am definitely not going to cover here other than to say that it tells us how well the factors correlate with each other. Since you are just learning about EFAs, disregard both of these for right now and just focus on the underlying concept of a factor and possibly think about how amazing it is that an algorithm and statistical approach can group variables together simply from raw response data.

### What if you only have a single factor?

A single factor tells you that your items likely fit onto a single theoretical construct which is just a fancy way of saying a single category. Operationally that means they can be represented on one dimension/scale which is nice and easy, but you can't get much information out of the items beyond what they tell you individually. But this is not necessarily a bad thing and you can use the output to your advantage, in that you can

1. use the factor analysis to help refine your scale like choosing only those items with higher loadings on the factor and removing those that are lower - i.e. by using some threshold to separate the higher values from the lower values you see on the arrows going from the factor to the items. 

2. see if any of your items could be reverse-scored on the scale by determining any negative loadings on the factor - i.e. negative values you see on the arrows going from the factor to the items. 

### Side Note: Lack of Pipes
So why can't we just use pipes and `ggplot` here? Well the main problem can be found using this command

```{r}
class(EFA)
```

Notice that the `psych` package has its own data type called  "psych". Tidy involves data that is neatly in rows and columns. In any case, we'll stick to the default format for now. 

You can actually use the `class()` syntax to figure out the data type for any variable in R.

Anyway getting back to the issue, we have three factors consisting of certain items. Now the final step is to go back and decide compartmentalize `MR1`, `MR2`, and `MR3` based on how the questions are grouped.

## Interpreting Factor Scores

### ...using Items

Let's take a look at the codebook, but this time look at them by items within factors

```{r}
MR1_factor <- conspiracy_codebook %>%
  filter(str_detect(Item, paste(c("Q12", "Q7", "Q11", "Q14", "Q1", "Q9", "Q2", "Q5"), collapse = '|')))

MR2_factor <- conspiracy_codebook %>%
  filter(str_detect(Item, paste(c("Q3", "Q13", "Q8", "Q4"), collapse = '|')))

MR3_factor <- conspiracy_codebook %>%
  filter(str_detect(Item, paste(c("Q6", "Q15", "Q10"), collapse = '|')))
```

If you're interested, the `|` is called a logical operator and it means the word ***or***. Information about other operators can be found [here](https://www.statmethods.net/management/operators.html){target="_blank"}.

```{r}
reactable(MR1_factor,
          defaultPageSize = 11)

reactable(MR2_factor)

reactable(MR3_factor)
```

We could say that 

1. `MR1` = *information control by groups*
2. `MR2` = *hiding information from the public*
3. `MR3` = *acts of self-interest to suppress information* 

Of course there are many interpretations so yours may differ. Regardless the idea should always be in the context of your original purpose and research questions.

The additional benefit here is that these interpretations can serve as *natural groupings*, or ones that are derived from, rather than labeling them prior to a statistical analysis. 

### ...using Participants

An EFA contains information about factor scores for each participant in your data set These scores are indicators of how much or how little of the factor each participant likely possesses. Please note that factor scores are not calculated for respondents with missing data. That issue will be tackled in next week's walkthrough. Anyway let's take a look at our data by calculating the total scores.

To see an example, lets first look at the raw data for the first six participants

```{r}
head(conspiracy_data_10)
```

or if you prefer, we can show this using

```{r}
reactable(conspiracy_data_10)
```

We can then apply `rowSums()` command to see the total scores for these respondents.

```{r}
conspiracy_data_10 %>%
  head() %>%
  rowSums()
```

Then let's look at the first six participant factor scores to see the relationship between those and the responses.

```{r}
head(EFA$scores)
```

and with their descriptive summary
```{r}
summary(EFA$scores)
```

I can plot the density curve
```{r}
plot(density(EFA$scores, 
             na.rm = TRUE), 
     main = "Factor Scores")
```

to get a feel for how the data looks. So what does it all mean? Well this was an example of how to go about performing one so we used a very small subset of the entire population for the purposes of shwoing an example. You should always perform an EFA in your entire data set where applicable, unless there is an underlying reason not to. 

However interpretations aren't conducted at the individual level, rather they are generalized across a larger sample. This means creating overarching statements about any of these ten participants separately would be complete nonsense, rather doing this can provide indicators of what you may see when implementing an EFA on the entire sample. 

If you want to know what the study said broadly, please take a look over the Brotherton, French & Pickering (2013) paper. 

## Caveaut
OK so we've conducted an EFA, but what I didn't tell you is that there are some guidelines to think about before implementing one. For those who have not had a statistics course may consider skipping the first for now, though the basic idea of the *variance* is given in this footer^[We essentially want a way to describe the *spread of our data* from one end to another, or *dispersion*. We have some ways to do this such as mean, median, and mode but that doesn't tell us the whole picture since these are just ways to describe the average. They essentially do  not say anything about how far away from the mean, median, or mode is from the raw data since that varies - ergo the term *variance*.]

1. Each factor must only contain items explaining at least 10% of the variance in its respective factor.
2. Each factor is recommended to have at least three items.
3. Each factor must be interpreted in a reasonable and applicable way.
4  It is recommended that your factoring do not have items that appear in multiple factors with a similar magnitude - aka the measure of an item on the factor.

## Let's Stop Here
OK that's enough. Factor analysis can be quite useful, but it is also often overused and interpretations are made beyond the scope of EFAs can actually tell us. Again this has been simplified, though it may not feel like it right now. Regardless when we lose complexity, a lot of the additional assumptions, benefits, possible outcomes, side effects, etc are lost as well so I encourage you to further explore EFAs and after you get "comfortable" with them, take a look at confirmatory factor analyses (CFAs) as well.

## Looking Forward
Recall that a factor analysis can only be performed on a full set of data, but what if you don't have that? Its something we deal with all of the time and we'll talk about how to deal with missingness next week!
